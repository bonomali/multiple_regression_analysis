{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import math\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "import statsmodels.api as smstats\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_heatmap(df, title = ''):\n",
    "    ## heatmeap to see the correlation between features. \n",
    "    # Generate a mask for the upper triangle (taken from seaborn example gallery)\n",
    "    mask = np.zeros_like(df.corr())\n",
    "    #mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    plt.subplots(figsize = (24,16))\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "    sns.heatmap(df.corr(), \n",
    "                annot=True,\n",
    "                #mask = mask,\n",
    "                cmap = 'RdBu_r',\n",
    "                linewidths=0.1, \n",
    "                linecolor='white',\n",
    "                vmax = 1,\n",
    "                square=True,\n",
    "               fmt='.3f')\n",
    "    plt.title(title, y = 1.03,fontsize = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b212e3630a8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mcorr_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mcorr_cols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdependent_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mcorr_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorr_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m summary_columns = pd.MultiIndex.from_tuples([\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "dependent_var = 'X19'\n",
    "\n",
    "# Independent variables:\n",
    "cols = ['X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18']\n",
    "col_names = {\n",
    "    'X6': 'Product Quality X6',\n",
    "    'X7': 'E-Commerce X7',\n",
    "    'X8': 'Technical Support X8',\n",
    "    'X9': 'Complaint Resolution X9',\n",
    "    'X10': 'Advertising X10',\n",
    "    'X11': 'Product Line X11',\n",
    "    'X12': 'Salesforce Image X12',\n",
    "    'X13': 'Competitive Pricing X13',\n",
    "    'X14': 'Warranty & Claims X14',\n",
    "    'X15': 'New Products X15',\n",
    "    'X16': 'Order & Billing X16',\n",
    "    'X17': 'Price Flexibility X17',\n",
    "    'X18': 'Delivery Speed X18',\n",
    "    'X19': 'Customer Satisfaction X19'\n",
    "}\n",
    "\n",
    "corr_cols = cols.copy()\n",
    "corr_cols.insert(0, dependent_var)\n",
    "corr_df = df[corr_cols]\n",
    "\n",
    "summary_columns = pd.MultiIndex.from_tuples([\n",
    "        ('', 'Step'),\n",
    "        ('', 'Var.'),\n",
    "        ('Overall Model Fit', 'R'),\n",
    "        ('Overall Model Fit', 'Rˆ2'),\n",
    "        ('Overall Model Fit', 'Adjusted Rˆ2'),\n",
    "        ('Overall Model Fit', 'Std. Error of the Estimate'),\n",
    "        ('Rˆ2 Change Statistics','Rˆ2 Change'),\n",
    "        ('Rˆ2 Change Statistics','F Value of Rˆ2 Change'),\n",
    "        ('Rˆ2 Change Statistics','df1'),\n",
    "        ('Rˆ2 Change Statistics','df2'),\n",
    "        ('Rˆ2 Change Statistics','Significance of Rˆ2 Change'),\n",
    "    ])\n",
    "\n",
    "model_summary = pd.DataFrame(columns=summary_columns)\n",
    "\n",
    "model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary of Stepwise Multiple Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME - \"stde_est\" It should be passed as argument\n",
    "#FIXME - \"model_summary\" should not be global\n",
    "#TODO FIXME - F Value of Rˆ2 Change\n",
    "\n",
    "def update_model_summary(OLSResults, vars_entered_into_rm, ve):\n",
    "    \n",
    "    #Overall Model Fit\n",
    "    r = round(math.sqrt(OLSResults.rsquared),3)\n",
    "    rsquared = round(OLSResults.rsquared,3)\n",
    "    rsquared_adj = round(OLSResults.rsquared_adj,3)\n",
    "\n",
    "    #Rˆ2 Change Statistics\n",
    "    step = model_summary.shape[0]+1\n",
    "    if step > 1:\n",
    "        previous_rsquared = model_summary.loc[step-1]['Overall Model Fit']['Rˆ2']\n",
    "        r2_change = round((rsquared - previous_rsquared),3)\n",
    "    else:\n",
    "        r2_change = round(rsquared,3)\n",
    "        \n",
    "    f_value_r2_change = round(OLSResults.fvalue,3) #FIXME\n",
    "    df1 = OLSResults.df_model\n",
    "    df2 = OLSResults.df_resid\n",
    "    sign_of_r2_change = vars_entered_into_rm.loc[ve]['P>|t|']\n",
    "\n",
    "    model_summary.loc[step] = (step,ve,r,rsquared,rsquared_adj, round(stde_est, 3), r2_change, f_value_r2_change, df1, df2, sign_of_r2_change)\n",
    "    \n",
    "model_summary    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colinearity diagnosis (Tolerance and VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vif_calc(x_vars):\n",
    "    vif_tol = pd.DataFrame(columns=['Tolerance', 'VIF'])\n",
    "\n",
    "    xvar_names = x_vars.columns\n",
    "    for col in xvar_names:\n",
    "        y = x_vars[col] \n",
    "        x = x_vars[xvar_names.drop(col)]\n",
    "        x = add_constant(x)\n",
    "\n",
    "        rsq = sm.OLS(y,x).fit().rsquared  \n",
    "        \n",
    "        vif = round(1/(1-rsq),3)\n",
    "        tolerancy = round(1-rsq, 3)\n",
    "\n",
    "        vif_tol.loc[col] = (tolerancy, vif)\n",
    "        \n",
    "    return vif_tol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS (Ordinary Least Squares Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLS(y,x):\n",
    "    df = corr_df.copy()\n",
    "    X = df[x]; \n",
    "    y = df[y]\n",
    "    \n",
    "    X = add_constant(X)\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables Entered the into Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variables_entered(dependent_var, varse):\n",
    "    df = corr_df.copy()\n",
    "    \n",
    "    results = OLS(dependent_var, varse)\n",
    "    vif_tol = vif_calc(df[varse])\n",
    "\n",
    "    rs = results.summary()\n",
    "\n",
    "    '''Note that tables is a list. The table at index 1 is the \"core\" table. \n",
    "       Additionally, read_html puts dfs in a list, so we want index 0\n",
    "    '''\n",
    "    results_as_html = rs.tables[1].as_html()\n",
    "    table1 = pd.read_html(results_as_html, header=0, index_col=0)[0]\n",
    "\n",
    "    return pd.concat([table1,vif_tol], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova(y, varse):\n",
    "    df = corr_df.copy()\n",
    "    X = add_constant(df)\n",
    "    formula = y + '~' + '+'.join(varse)\n",
    "    mod = sm.ols(formula, data=X).fit()\n",
    "    \n",
    "    aov_table = smstats.stats.anova_lm(mod, typ=2)\n",
    "    aov_table = round(aov_table,3)\n",
    "    \n",
    "    return aov_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infl = results.get_influence()\n",
    "# df_infl = infl.summary_frame()\n",
    "# round(df_infl[:5],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial correlation\n",
    "- First order\n",
    "![title](first_order.png)\n",
    "- High order\n",
    "![title](high_order.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "columns = pd.MultiIndex.from_tuples([\n",
    "        ('', 'Beta In'),\n",
    "        ('Statistical Significance', 't-value'), ('Statistical Significance', 'Significance'),\n",
    "        ('', 'Partial Correlation'),\n",
    "        ('Collinearity Statistics', 'Tolerance'),('Collinearity Statistics', 'VIF')\n",
    "    ])\n",
    "\n",
    "zero_order_corr = corr_df.corr()\n",
    "\n",
    "def calc_abc(ab,ac,bc):\n",
    "    return (ab-ac*bc) / (np.sqrt(1-pow(ac, 2)) * np.sqrt(1-pow(bc, 2)))\n",
    "\n",
    "def first_order(a, b, c):\n",
    "    ra = zero_order_corr.loc[a,b] \n",
    "    rb = zero_order_corr.loc[a,c] \n",
    "    rc = zero_order_corr.loc[b,c]\n",
    "\n",
    "    return calc_abc(ra,rb,rc)\n",
    "\n",
    "def partial_correlation(j,k,p,varse):\n",
    "    if(len(varse)==1):\n",
    "        return first_order(j,k,p)\n",
    "    \n",
    "    return calc_abc(\n",
    "        partial_correlation(j, k, varse[-2], varse[:-1]),\n",
    "        partial_correlation(j, varse[-1], varse[-2], varse[:-1]),\n",
    "        partial_correlation(k, varse[-1], varse[-2], varse[:-1])\n",
    "    )\n",
    "\n",
    "def partial_correlations(j, k, varse):\n",
    "    rjkp = partial_correlation(j, k, varse[0], varse.copy())\n",
    "\n",
    "    return round(rjkp,3)\n",
    "\n",
    "def vif_in(X, v):  \n",
    "    p = pd.Series(variance_inflation_factor(X.values, 1), index=X.columns)\n",
    "\n",
    "    vif = p[v]\n",
    "    tolerance = 1/vif\n",
    "    \n",
    "    return (round(tolerance,3), round(vif,3))\n",
    "\n",
    "def beta_stats_sign(Y, X, k):\n",
    "    df = corr_df.copy()\n",
    "    \n",
    "    model = sm.OLS(Y, X)\n",
    "    result = model.fit()\n",
    "\n",
    "    #beta\n",
    "    beta_in = result.params[k]\n",
    "\n",
    "    #t, p-value, dof\n",
    "    t = result.tvalues[k]\n",
    "    p = result.pvalues[k]\n",
    "    dof = df.shape[0]-2\n",
    "    significance = stats.t.sf(abs(t), dof)*2\n",
    "\n",
    "    return (round(beta_in,3), round(t,3), round(significance,3))\n",
    "\n",
    "def highlight_max(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    if(s.name[1] in ['Significance', 'VIF']):\n",
    "        is_min = s == s.min() \n",
    "\n",
    "        return ['background-color: yellow' if v else '' for v in is_min]\n",
    "    else:\n",
    "        is_max = s == s.max()\n",
    "        \n",
    "        return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "def variables_not_entered_int_rm(y, varse):\n",
    "    df = corr_df.copy()\n",
    "    \n",
    "    stats_sign = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # standardizing dataframe\n",
    "    df_z = df.select_dtypes(include=[np.number]).dropna().apply(stats.zscore)\n",
    "    \n",
    "    for k in df_z.columns:\n",
    "        if(k == y or k in varse):\n",
    "            continue   \n",
    "            \n",
    "        Y = df_z[y]\n",
    "        X = add_constant(df_z[[k]+varse])\n",
    "        \n",
    "        #Beta and Statistical significance\n",
    "        beta_in, t, sig = beta_stats_sign(Y,X,k)\n",
    "        \n",
    "        #Tolerance, Vif\n",
    "        tolerance,vif = vif_in(X, k)\n",
    "        \n",
    "        #Partial correlation\n",
    "        partial_correlation = partial_correlations(y, k, varse)\n",
    "\n",
    "        stats_sign.loc[col_names[k]] = (beta_in,t,sig, partial_correlation, tolerance, vif)\n",
    "        \n",
    "    return stats_sign.style.apply(highlight_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r12 = .603 #X19 and X9\n",
    "# r13 = .486 #X19 and X6\n",
    "# r23 = .106 #X9 and X6\n",
    "\n",
    "# partial = (r12-r13*r23) / (np.sqrt(1-pow(r13, 2)) * np.sqrt(1-pow(r23, 2)))\n",
    "# print(\"X9 partial: \", round(partial, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x9_semipartial_corr = (r12-r13*r23) / np.sqrt(1-pow(r23, 2))\n",
    "# x9_semipartial_corr = round(x9_semipartial_corr, 3)\n",
    "# print(\"Semipartial of X9 and X19 with X6 controlled: \", x9_semipartial_corr)\n",
    "# print(\"Unique variance previewd X9: \", round(pow(x9_semipartial_corr, 2),3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x6_semipartial_corr = (r12-r13*r23) / np.sqrt(1-pow(r13, 2))\n",
    "\n",
    "# x6_semipartial_corr = round(x6_semipartial_corr, 3)\n",
    "# print(\"Semipartial of X6 and X19 with X9 controlled: \", x6_semipartial_corr)\n",
    "# print(\"Unique variance previewd X6: \", round(pow(x6_semipartial_corr, 2),3) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl = pd.ExcelFile('Multivariate_Data_Analysis_6e_Datasets_EXCEL.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xl.sheet_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = xl.parse('HBAT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data_dictionary.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](stage1_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Objectives of Multiple Regression \n",
    "\n",
    "The following 13 variables were included as independent variables:\n",
    "\n",
    "We selecte **Customer Satisfaction (X19)** as the dependent variable **(Y)** to predicted by independent variables. \n",
    "\n",
    "The relationship among the 13 independent variables and customer satisfaction was assumed to be statistical, not functional, because it involved perceptions of performance and may include levels of measurement error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](variables.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Research Design of a Multiple Regression Analysis\n",
    "\n",
    "- The first question to be answered concerning sample size is the level of relationship **(Rˆ2)** that can be detected reliably with the proposed regression analysis. \n",
    "\n",
    "- Table 5 indicates that the sample of 100, with 13 potential independent variables, is able to detect relationships with Rˆ2 values of approximately **23%** at power of **.80** with the significance level set at **.01**. If the significance level is relaxed to **.05**, then the analysis will identify relationships explaining about **18%** of the variance. \n",
    "\n",
    "- The sample of 100 observartions also meets the guideline for the minimum ratio of observations to indepent variables (**5:1**) with an actual ratio of **7:1** (100 observations with 13 variables)\n",
    "\n",
    "    The proposed regression analysis was deemed sufficient to identify not only statistically significant relationships but also relationships that had managerial significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](minimum_r2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: Assumptions in Multiple Regression Analysis\n",
    "\n",
    "Analysis to ensure the research is meeting the basic assumptions of regression analysis involves two steps:\n",
    "\n",
    "    1 - Testing the individual dependent and independent variables\n",
    "    2 - Testing the overall relationship after model estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3.1.1 - Scatterplots to verify non-linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,15))\n",
    "fig.subplots_adjust(hspace=0.8, wspace=0.8)\n",
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    ax = fig.add_subplot(4, 4, idx+1)\n",
    "    sns.scatterplot(x = corr_df[col], y=corr_df[dependent_var], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scatterplots of the individual variables did not indicate any nonlinear relationships between the dependent **(X19)** and the independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3.1.1 -  Checking normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import qqplot\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "fig.subplots_adjust(hspace=0.8, wspace=0.8)\n",
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    ax = fig.add_subplot(4, 4, idx+1)\n",
    "    qqplot(df[col], line='s', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,15))\n",
    "fig.subplots_adjust(hspace=0.8, wspace=0.8)\n",
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    ax = fig.add_subplot(4, 4, idx+1)\n",
    "    sns.distplot(df[col], fit=stats.norm, ax=ax);\n",
    "    stats.probplot(df[col])    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### skewness and kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk = pd.DataFrame(columns=['Skewness', 'Kurtosis'], index=cols)\n",
    "for col in cols:\n",
    "    #print(\"{}\".format(first, second))\n",
    "    skewness = corr_df[col].skew()\n",
    "    kurtosis = corr_df[col].kurt()\n",
    "    sk.loc[col] = (round(skewness,3),round(kurtosis,3))\n",
    "sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Six variables were found to violate the statistical tests\n",
    "    * **X6** \n",
    "    * **X7**\n",
    "    * **X12**\n",
    "    * **X13**\n",
    "    * **X16**\n",
    "    * **X17**\n",
    "    \n",
    "- For all, except one **(X12)**, transformations were sufficient remedies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3.1.2 - Checking heteroscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, col in enumerate(cols):\n",
    "    sns.jointplot(x=corr_df[col], y=corr_df[dependent_var], kind='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Only two variables **(X6 and X17)** had minimal violations of this assumption, with no corrective action needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Although regression analysis has been shown to be quite robust even when the normality assumption is violated, researches should estimate the regression analysis with both the original and transformed variables to assess the consequences of nonnormality of the independent variables on the interpretation of the results. \n",
    "\n",
    "* Here we use the original variables and later results for the transformed variables are shown for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](stage4_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 4: Estimating the Regression Model and Assessing Overall Model Fit\n",
    "\n",
    "- With the regression analysis specified in terms of dependent and independent variables, the sample deemeded adequate for the objectives of the study, and the assumptions assessed for the individual variables, the model-building process now proceeds to estimation of the regression model and assessing the overall model fit.\n",
    "\n",
    "- For purposes of illustration, the stepwise procedure is employed to select variables for inclusion in the regression variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](stepwise_estimation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepwise estimation: Selecting the first variable\n",
    "\n",
    "- The stepwise estimation procedure maximizes the incremental **explained variance** at each step of model building.\n",
    "- In the first step, the **highest bivariate correlation** (also the **highest partial correlation**, because no other variable are in the equation) will be selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_heatmap(corr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Examination of the correlation matrix reveals that **compaint resolution (X9)** has the highest bivariate correlation with the dependent variable **X19** (**0,603**). \n",
    "- The first step is to build a regression equation using just this single independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varse = ['X9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = OLS(dependent_var, varse)\n",
    "results.summary().tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stde_est = np.sqrt(results.ssr / results.df_resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova(dependent_var, varse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_entered_into_rm = variables_entered(dependent_var, varse)    \n",
    "vars_entered_into_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_model_summary(results, vars_entered_into_rm, 'X9')\n",
    "model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Variables Entered into the Regression Model\n",
    "\n",
    "- **Multiple R** is the correlation  coefficient (at this step) for the simple regression of **X9 and X19**. It has no plus or minus sign because in multiple regression the signs of the individual variables may vary, so this coefficient reflects only the degree of association. In the first step of the stepwise estimation, the Multiple R is the same as the bivariate correlation (**.603**) because the equation contains only one variable\n",
    "- **R-squared** is the correlation coefficient squared (**.603ˆ2 = .364**), also referred to as the *coefficient of determination*. This value indicates the percentage of total variation of Y (*X19, Customer Satisfaction*) explained by the regression model consisting of **X9**\n",
    "- **Standard Error of the Estimate** is another measure of the accuracy of our predictions. It is the **sqrt(sum_sq)/df**, also represented by the sqrt of the ***MSresidual***. \n",
    "    - It represents an estimate of the standard deviation of the actual dependent values around the regression line; that is, it is a measure of variation around the regression line. Also can be viewed as the standard deviation of the prediction errors; thus it becomes a measure to assess the absolute size of the prediction error. It is used also in estimating the size of the confidence interval for the predictions\n",
    "- ***ANOVA and F Ratio***  The *ANOVA* analysis provides the statistical test for the overall model fit in terms of the *F ratio*. \n",
    "    - The total sum of squares is the squared error that would ocurr if we used only the mean of Y to predict the dependent variable. Using the values of *X9* reduces this error by **36.4 percent** (51.178/140.628). This reduction is deemed statistically significant with an ***F ratio*** of **56.070** and a **significance level of .000**    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Variables in the Equation (Step 1)** In step 1, a single independent variable **(X9)** is used to calculate the regression equation for prediction the dependent variable. For each variable in the equation, several measures need to be defined: \n",
    "    - **The regression coefficient (b)**: Reflect the change in the dependent measure for each unit change in the Y. Comparison between regression coefficients allows for a relative assessment of each variable's importance in the regression model. The value **.595** is the regression coefficient *(**b9**)* for the **Y (X9)**. The predicted value for each observation in the intercept **(*Y* = 3.680 + .595X9)**.\n",
    "    \n",
    "    - **The standard error of the coefficient**: In a simple sense, it is the standard deviation of the estimates of ***b9*** across multiple samples. A smaller standard error implies more reliable prediction and therefore smaller confidence intervals. \n",
    "        - The standard error of ***b9*** is **.079**, denoting that the **95%** confidence interval for ***b9*** would be **.595 ± (1.96 x .079)**, or ranging from a low of **.44** to a high of **.75**. \n",
    "        - The value of ***b9*** divided by the standard error **(.595/.079 = 7.488)** is the calculated ***t*** value for a ***t-test*** of the hypothesis ***b9* = 0**\n",
    "    - **The *t* value of variables in the Equation**: as just calculated, measures the significance of the partial correlation of the variable reflected in the regression coefficient. As such, it indicates whether the researcher can confidently say, with a stated level of error, that the coefficient is not equal to zero. ***F*** values may be given at this stage rather than ***t*** values.\n",
    "        - They are directly comparable beacause the ***t*** value is approximately the square root of the ***F value***.\n",
    "        - Is also particularly useful in the stepwise procedure in helping to determine whether any variable should be dropped from the equation once another independent variable has been added. The calculated level of significance is compared to the threshold level set by the researcher for dropping the variable. In our example, we set a **.10** level for dropping variables from the equation. \n",
    "        - The critical value for a significance level of **.10** with **98 df** is **1.658**. As more variables are added to the regression equation, each variable is checked to see whether it still falls within this threshold. If it falls outside the threshold (**significance > .10**), it is eliminanted from the regression equation, and the model is estimated again\n",
    "        - In our example, the ***t*** value (as derived by dividing the regression coefficient by the standard error) is **7.488**, which is statistically significant at the **.000** level. It gives the researcher a high level of assurance that the coefficient is not equal to zero and can be assessed as a predictor of customer satisfaction.\n",
    "        \n",
    "    - ***Correlations*** Three different correlations are given as an aid in evaluating the estimation process. \n",
    "        - The ***zero-order*** correlation is the simple bivariate correlation between the independent and dependent variable. \n",
    "        - The ***partial correlation*** denotes the incremental predictive effect, controlling for other variables in the regression model on both dependent and independent variables. This measure is used for judging which variable is next added in sequential search methods. \n",
    "        - Finally, the ***part correlation*** denotes the unique effect attributable to each independent variable.\n",
    "        - For the first step in a stepwise solution, all three correlations are identical (**.603**) because no other variables are in the equation. As variables are added, these values will differ, each reflecting their perspective on each independent variable’s contribution to the regression model.    \n",
    "    - **The collinearity diagnosis (tolerance and VIF):** provide a perspective on the impact of collinearity on the independent variables in the regression equation. In the case of a single variable in the regression model, the tolerance is **1.00**, indicating that it is totally unaffected by other independent variables (as it should be since it is the only variable in the model). Also, the VIF is **1.00**. Both values indicate a complete lack of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Variables Not Entered into the Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_notent_into_rm = variables_not_entered_int_rm(dependent_var, varse)\n",
    "vars_notent_into_rm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Variables Not Entered into the Regression Model\n",
    "\n",
    "- With *X9* included in the regression equation, 12 other potential independent variables remain for inclusion to improve the prediction of the dependent variable. For each of these variables, four types of measures are available to assess their potential contribution to the regression model:\n",
    "    \n",
    "    - **Partial correlations and Collinearity Measures**: is a measure of the variation in Y that can be accounted for by each of these additional variables, controlloing for the variables already in the equation (only *X9* in step 1). If the variable with the highest partial correlation exceeds the threshold of statistical significance required for inclusion, it will be added to the regression model at the next step. The partial correlation represents the correlation of each variable in the model with the unexplained portion of dependent variable. As such, the contribution of the partial correlation (the squared partial correlation) is that percentage of the unexplained variance that is explained by the addicion of this independent variable. \n",
    "    \n",
    "    It is interesting to note, however, that\n",
    "        - ***X6*** had only the **sixth highest bivariate** correlation with *X19*. Why was it the second variable to enter the stepwise equation, ahead of the variables with higher correlations? \n",
    "        - The variables with the **second highest correlations with X19** were ***X18*** (**.577**), \n",
    "        - **Third**, ***X11*** (**.551**) \n",
    "        - **Fourth**, ***X16*** (**.522**) \n",
    "        - Both *X18* and *X16* had high correlations with X9, reflected in their rather low tolerance values of .252 and .427, respectively. It should be noted that this fairly high level of multicollinearity is not unexpected, because these three variables (X9, X16, and X18) constituted the first factor that would be derived in a first factor analysis (not performed here). X11, even though it does not join this factor, is highly correlated with X9 (.561) to the extent that the tolerance is only .685. \n",
    "        - Finally, ***X12***, the **fifth highest bivariate** correlation with X19, only has a correlation with X9 of **.230**, but it was just enough to make the partial correlation slightly lower than that of X6. The correlation of X9 and X6 of only **.106** resulted in a tolerance of **.989** and transformed the bivariate correlation of **.486** into a partial correlation of **.532**, which was highest among all the remaining 12 variables.\n",
    "        \n",
    "        If *X6* is added, then the R2 value should increase by the partial correlation squared times the amount of unexplained variance **(change in R2 = .532ˆ2 * .636 = .180)**. Because **36.4** percent was already explained by *X9*, *X6* can explain only **18.0** percent of the remaining variance. \n",
    "    \n",
    "    - **Standardized coefficients (Beta)** \n",
    "        - We see that ***X6***, the variable with the highest partial correlation, also has the highest Beta coefficient if entered. Even though the magnitude of **.427** is substantial, it can also be compared with the beta for the variable now in the model (***X9*** with a **beta** of **.603**), indicating that *X6* will make a substantive contribution to the explanation of the regression model, as well as to its predictive capability.\n",
    "    \n",
    "    - ***t* values** The *t* value measures the significance of the partial correlations for variables not in the equation. They are calculated as a ratio of the additional sum of squares explained by including a particular variable and the sum of squares left after adding that same variable. If this t value does not exceed a specified significance level (e.g., .05), the variable will not be allowed to enter the equation.\n",
    "        \n",
    "        - We note that six variables *(X6, X7, X10, X11, X12, and X17)* exceed this value and are candidates for inclusion. Although all are significant, the variable added will be that variable with the highest partial correlation. We should note that establishing the threshold of statistical significance before a variable is added precludes adding variables with no significance even though they increase the overall R2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking Ahead** With the first step of the stepwise procedure completed, the final task is to evaluate the variables not in the equation and determine whether another variable meets the criteria and can be added to the regression model. As noted earlier, the partial correlation must be great enough to be statistically significant at the specified level (generally .05). If two or more variables meet this criterion, then the variable with the highest partial correlation is selected.\n",
    "\n",
    "As described earlier, **X6 (Product Quality**) has the highest partial correlation at this stage, even though four other variables had higher bivariate correlations with the dependent variable. In each instance, multicollinearity with X9, entered in the first step, caused the partial correlations to decrease below that of X6.\n",
    "\n",
    "We know that a significant portion of the variance in the dependent variable is explained by X9, but the stepwise procedure indicates that if we add X6 with the highest partial correlation coeffi- cient with the dependent variable and a ***t value*** is **significant at the .05 level**, we will make a significant increase in the predictive power of the overall regression model. Thus, we can now look at the new model using both X9 and X6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepwise estimation: Selecting the second variable (X6)\n",
    "\n",
    "The next step in a stepwise estimation is to check and delete any of the variables in the equation that now fall below the significance threshold, and once done, add the variable with the **highest statistically significant partial correlation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varse.append('X6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = OLS(dependent_var, varse)\n",
    "results.summary().tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stde_est = np.sqrt(results.ssr / results.df_resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova(dependent_var, varse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_entered_into_rm = variables_entered(dependent_var, varse)    \n",
    "vars_entered_into_rm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Estimated Coefficients** \n",
    "    - The beta weight is **.427**. \n",
    "    - Although not as large as the beta for X9 **(.558)**, X6 still has a substantial impact in the overall regression model. \n",
    "    - The coefficient is statistically significant and multicollinearity is minimal with X9 (as described in the earlier section). \n",
    "    - Thus, tolerance is quite acceptable with a value of **.989** indicating that only **1.1** percent of either variable is explained by the other.\n",
    "    \n",
    "    \n",
    "- **Impact of Multicollinearity** The lack of multicollinearity results in little change for either the value of b9 **(.550)** or the beta of X9 **(.558)** in step 1. It further indicates that variables X9 and X6 are relatively independent (the simple correlation between the two variables is **.106**). If the effect of X6 on Y were totally independent of the effect of X9, the b9 coefficient would not change at all. \n",
    "    - The ***t values*** indicate that both X9 and X6 are statistically significant predictors of Y. \n",
    "    - The ***t value*** for X9 is now **8.092**, whereas it was **7.488** in step 1. \n",
    "    - The ***t value*** for X6 relates to the contribution of this variable given that X5 is already in the equation. Note that the t value for X6 (**6.193**) is the same value shown for X6 in step 1 under the heading “Variables Not Entered into the Regression Model”\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_model_summary(results, vars_entered_into_rm, 'X6')\n",
    "model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Overall Model Fit** The **multiple R** and **R2** values have both increased with the addition of **X6**. \n",
    "    - The **R2** increased by **18.0 percent**\n",
    "    - Then, of the **63.3** percent unexplained with X9, **(.532)ˆ2** of this variance was explained by adding X6, yielding a **total variance explained (R2)** of **.544**. \n",
    "    - The **adjusted R2** increased to **.535** \n",
    "    - The standard error of the estimate decreased from **.955 to .813**. \n",
    "    \n",
    "    Both of these measures also demonstrate the improvement in the overall model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_notent_into_rm = variables_not_entered_int_rm(dependent_var, varse)\n",
    "vars_notent_into_rm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Identifying Variables to Add** \n",
    "    - Looking at the partial correlations for the variables not in the equation, we see that: \n",
    "        - **X12** has the highest partial correlation (**.676**), which is also statistically significant at the **.000** level. \n",
    "        - Would explain **45.7** percent of the heretofore unexplained variance (**.676ˆ2 = .457**), or **20.9** percent of the total variance (**.676ˆ2 * .456**). \n",
    "\n",
    "This substantial contribution actually slightly surpasses the incremental contribution of X6, the second variable entered in the stepwise procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepwise estimation: A third variable (X12) is added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varse.append('X12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = OLS(dependent_var, varse)\n",
    "results.summary().tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stde_est = np.sqrt(results.ssr / results.df_resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova(dependent_var, varse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_entered_into_rm = variables_entered(dependent_var, varse)   \n",
    "vars_entered_into_rm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Estimated Coefficients** \n",
    "    The addition of X12 brought a third statistically significant predictor of customer satisfaction into the equation. \n",
    "    - is complemented by a **beta weight** of **.477**, second highest among the three variables in the model (behind the **.512** of X6).\n",
    "    \n",
    "- **Impact of Multicollinearity** It is noteworthy that even with the third variable in the regression equation, multicollinearity is held to a minimum. \n",
    "    - The lowest tolerance value is for **X12** (**.916**), indicating that only **8.4** percent of variance of **X12** is accounted for by the other two variables. This pattern of variables entering the stepwise procedure should be expected, however, when viewed in light of a preliminary factor analysis. \n",
    "    \n",
    "    From those results, we would see that the three variables now in the equation (**X9**, **X6**, and **X12**) are each members of different factors in that analysis. Because variables within the same factor exhibit a high degree of multicollinearity, it would be expected that when one variable from a factor enters a regression equation, the odds of another variable from that same factor entering the equation are rather low (and if it does, the impact of both variables will be reduced due to multicollinearity).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_model_summary(results, vars_entered_into_rm, 'X12')\n",
    "model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Overall Model Fit** As predicted, the value of **R2** increases by **20.9** percent (.753 - .544 = .209). Moreover, the **adjusted R2** increases to **.745** and the **standard error of the estimate decreases to .602**. Again, as was the case with **X6** in the previous step, the new variable entered (**X12**) makes substantial contribution to overall model fit.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_notent_into_rm = variables_not_entered_int_rm(dependent_var, varse)\n",
    "vars_notent_into_rm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Identifying Variables to Add** At this stage in the analysis, only three variables (**X7**, **X11**, and **X18**) have the statistically significant partial correlations necessary for inclusion in the regression equation. \n",
    "    - What happened to the other variables’ predictive power? By reviewing the bivariate correlations of each variable with X19 in Table 7, we can see that of the 13 original independent variables, three variables had nonsignificant bivariate correlations with the dependent variable (**X8**, **X15**, and **X17**).\n",
    "    - Thus X10, X13, X14, and X16 all have significant bivariate correlations, yet their partial correlations are now nonsignificant. \n",
    "        - For **X16**, the high bivariate correlation of **.522** was reduced markedly by high multicollinearity (tolerance value of **.426**, denotes that less than half of original predictive power remaining). \n",
    "        - For the other three variables, **X10**, **X13**, and **X14**, their lower bivariate correlations (**.305**, **-.208**, and **.178**) have been reduced by multicollinearity just enough to be nonsignificant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepwise estimation: A fourth variable (X7) is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varse.append('X7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = OLS(dependent_var, varse)\n",
    "results.summary().tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stde_est = np.sqrt(results.ssr / results.df_resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova(dependent_var, varse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_entered_into_rm = variables_entered(dependent_var, varse)    \n",
    "vars_entered_into_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_model_summary(results, vars_entered_into_rm, 'X7')\n",
    "model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_notent_into_rm = variables_not_entered_int_rm(dependent_var, varse)\n",
    "vars_notent_into_rm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepwise estimation: A fifth variable (X11) is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varse.append('X11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = OLS(dependent_var, varse)\n",
    "results.summary().tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stde_est = np.sqrt(results.ssr / results.df_resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova(dependent_var, varse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_entered_into_rm = variables_entered(dependent_var, varse) \n",
    "vars_entered_into_rm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Impact of Multicollinearity** The impact of multicollinearity, even among just these five variables, is substantial. \n",
    "    - Of the five variables in the equation, three of them (X12, X7, and X11) have **tolerance** values less than **.50** indicating that over one-half of their variance is accounted for by the other variables in the equation. Moreover, these variables were the last three to enter in the stepwise process.\n",
    "    - If we examine the **zero-order** (bivariate) and **partial correlations**, we can see more directly the effects of multicollinearity. \n",
    "        - For example, **X11** has the third highest bivariate correlation (**.551**) among all 13 variables, yet multicollinearity (**tolerance** of **.492**) reduces it to a partial correlation of only **.135**, making it a marginal contributor to the regression equation. \n",
    "        - In contrast, **X12** has a bivariate correlation (**.500**) that even with high multicollinearity (tolerance of **.347**) still has a partial correlation of **.411**. Thus, multicollinearity will always affect a variable’s contribution to the regression model, but must be examined to assess the actual degree of impact.\n",
    "    - If we take a broader perspective, the variables entering the regression equation correspond almost exactly to the factors derived in a first factor analysis (not performed here). \n",
    "        - **X9** and **X6** are each members of separate factors, with multicollinearity reducing the partial correlations of other members of these factors to a nonsignificant level. \n",
    "        - **X12** and **X7** are both members of a third factor, but multicollinearity caused a change in the sign of the estimated coefficient for **X7** (see a more detailed discussion in stage 5). \n",
    "        - Finally, **X11** did not load on any of the factors, but was a marginal contributor in the regression model.\n",
    "\n",
    "- The impact of multicollinearity as reflected in the factor structure becomes more apparent in using a stepwise estimation procedure and will be discussed in more detail in stage 5. Even apart from issues in explanation, however, multicollinearity can have a substantial impact on the overall predictive ability of any set of independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_model_summary(results, vars_entered_into_rm, 'X11')\n",
    "model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Overall Model Fit** The final regression model with five independent variables explains almost **80 percent** of the variance of **customer satisfaction (X19)**. \n",
    "    - The **adjusted R2** of **.780** indicates no overfitting of the model and that the results should be generalizable from the perspective of the ratio of observations to variables in the equation (20:1 for the final model). \n",
    "    - The **standard error** of the estimate has been **reduced to .559**, which means that at the **95% confidence level** (±1.96 * standard error of the estimate), the margin of error for any predicted value of X19 can be calculated at ±1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_notent_into_rm = variables_not_entered_int_rm(dependent_var, varse)\n",
    "vars_notent_into_rm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- Each of the first three variables added to the equation made substantial contributions to the overall model fit, with substantive increases in the **R2** and **adjusted R2** while also decreasing the **standard error of the estimate**. With only the first three variables, **75 percent** of the variation in **customer satisfaction** is explained with a confidence interval of **±1.2**. \n",
    "- Two additional variables are added to arrive at the final model, but these variables, although statistically significant, make much smaller contributions. The **R2** increases by **3 percent** and the **confidence interval** **decreases to ±1.1**, an improvement of **.1**. The relative impacts of each variable will be discussed in stage 5, but the stepwise procedure highlights the importance of the first three variables in assessing overall model fit.\n",
    "\n",
    "- In evaluating the estimated equation, we considered statistical significance. We must also address two other basic issues: \n",
    "    - (1) meeting the assumptions underlying regression and \n",
    "    - (2) identifying the influential data points. We consider each of these issues in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 5: Interpreting the Regression Variate\n",
    "With the model estimation completed, the regression variate specified, and the diagnostic tests that confirm the appropriateness of the results administered, we can now examine our predictive equation based on five independent variables (X6, X7, X9, X11, and X12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
